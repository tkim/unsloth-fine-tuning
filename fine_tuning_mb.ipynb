{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    # For Google Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone the repository to get the data file\n",
    "    !git clone https://github.com/tkim/unsloth-fine-tuning.git /content/unsloth-fine-tuning\n",
    "    \n",
    "    # Set the correct path for Colab\n",
    "    csv_path = \"/content/unsloth-fine-tuning/data/training-data-1.csv\"\n",
    "else:\n",
    "    # For local environment\n",
    "    csv_path = \"data/training-data-1.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df)} rows from {csv_path}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"File not found at {csv_path}\")\n",
    "    print(\"Creating sample data...\")\n",
    "    # Create sample data if file doesn't exist\n",
    "    import numpy as np\n",
    "    sample_data = []\n",
    "    for i in range(100):\n",
    "        history = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        sample_data.append({\n",
    "            'input': ', '.join(map(str, history)),\n",
    "            'output': ', '.join(map(str, next_nums))\n",
    "        })\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"Created sample data with {len(df)} rows\")\n",
    "    print(df.head())"
   ],
   "metadata": {
    "id": "-dHQkCGtFWa7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \u26a1 Quick Start Instructions\n",
    "\n",
    "To run this notebook successfully:\n",
    "\n",
    "1. **Runtime Setup**: Go to `Runtime` \u2192 `Change runtime type` \u2192 Select `GPU` (T4 or better)\n",
    "2. **Run All**: Click `Runtime` \u2192 `Run all` to execute all cells in order\n",
    "3. **Manual Execution**: If running cells individually, run them in order from top to bottom\n",
    "\n",
    "**Important**: The cells must be run in sequence. Each cell depends on variables created in previous cells.\n",
    "\n",
    "### \ud83d\udd27 Troubleshooting Common Issues:\n",
    "\n",
    "- **\"name 'df' is not defined\"**: Run the data loading cell first\n",
    "- **\"You cannot perform fine-tuning on purely quantized models\"**: Run the LoRA adapter cell before training\n",
    "- **Out of memory**: Reduce batch size or use a smaller model\n",
    "- **File not found**: The notebook will automatically create sample data if the CSV file is missing"
   ],
   "metadata": {
    "id": "FrVrnpx6FWa-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL3byIHfWEVx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "708b40de-79d5-428a-85b5-7fd20e3f124e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\u2713 Loaded 1847 rows from /content/unsloth-fine-tuning/data/training-data-1.csv\n",
      "\u2713 Columns: ['number_1', 'number_2', 'number_3', 'number_4', 'number_5', 'mb']\n",
      "\n",
      "First few rows:\n",
      "   number_1  number_2  number_3  number_4  number_5  mb\n",
      "0        20        36        37        48        67  16\n",
      "1        14        39        43        44        67  19\n",
      "2         9        38        47        49        68  25\n",
      "3        15        16        18        39        59  17\n",
      "4         5        11        25        27        64  13\n",
      "\n",
      "\u2713 Data ready for training: 1847 examples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    # For Google Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Clone the repository to get the data file (if not already cloned)\n",
    "    if not os.path.exists('/content/unsloth-fine-tuning'):\n",
    "        !git clone https://github.com/tkim/unsloth-fine-tuning.git /content/unsloth-fine-tuning\n",
    "\n",
    "    # Set the correct path for Colab\n",
    "    csv_path = \"/content/unsloth-fine-tuning/data/training-data-1.csv\"\n",
    "else:\n",
    "    # For local environment\n",
    "    csv_path = \"data/training-data-1.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\u2713 Loaded {len(df)} rows from {csv_path}\")\n",
    "    print(f\"\u2713 Columns: {df.columns.tolist()}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f File not found at {csv_path}\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    # Create sample data if file doesn't exist\n",
    "    sample_data = []\n",
    "    for i in range(100):\n",
    "        history = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        sample_data.append({\n",
    "            'input': ', '.join(map(str, history)),\n",
    "            'output': ', '.join(map(str, next_nums))\n",
    "        })\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"\u2713 Created sample data with {len(df)} rows\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "\n",
    "# Verify data is ready\n",
    "print(f\"\\n\u2713 Data ready for training: {len(df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOTWElUCWk_v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54854300-29e7-4644-b2fd-797b8bf1935f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x953lw83WxnY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "019f2947-7f94-48aa-c2b5-bbbdb26526e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.7.7: Fast Mistral patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v08de3wAXdu6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "15602784-4b9a-44ce-8de1-ab7723605a4c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created dataset with 1847 examples\n",
      "Sample formatted prompt:\n",
      "### Input: \n",
      "### Output: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Check if df is defined\n",
    "if 'df' not in globals():\n",
    "    print(\"Error: DataFrame 'df' is not defined. Please run the data loading cell first.\")\n",
    "    print(\"Creating sample data...\")\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    sample_data = []\n",
    "    for i in range(100):\n",
    "        history = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        sample_data.append({\n",
    "            'input': ', '.join(map(str, history)),\n",
    "            'output': ', '.join(map(str, next_nums))\n",
    "        })\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"Created sample data with {len(df)} rows\")\n",
    "\n",
    "def format_prompt(row):\n",
    "    \"\"\"\n",
    "    Format CSV row data for number combination prediction.\n",
    "    Adjust column names based on your CSV structure.\n",
    "    \"\"\"\n",
    "    # Example format - adjust based on your CSV columns\n",
    "    # Assuming columns like 'history', 'pattern', 'next_numbers'\n",
    "    if 'input' in row and 'output' in row:\n",
    "        return f\"### Input: {row['input']}\\n### Output: {row['output']}<|endoftext|>\"\n",
    "    else:\n",
    "        # Adjust this based on your actual CSV columns\n",
    "        # For number prediction, you might have historical numbers as input\n",
    "        # and the next numbers as output\n",
    "        input_text = str(row.get('history', row.get('previous_numbers', '')))\n",
    "        output_text = str(row.get('next_numbers', row.get('prediction', '')))\n",
    "        return f\"### Input: {input_text}\\n### Output: {output_text}<|endoftext|>\"\n",
    "\n",
    "# Convert DataFrame to formatted dataset\n",
    "formatted_data = [format_prompt(row) for _, row in df.iterrows()]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Sample formatted prompt:\\n{formatted_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZrtr0c4XmTE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "3288a8ea33c24051a44e2944000f0441",
      "0e5a8c634d6644eb9a6a56f9263ac3f9",
      "a45c56f2461a4fc8b3d9d7e093678fdf",
      "c2edb12c571a4ea5852528ad9cf30330",
      "4a4e119827fa4beebba0a7985c236b6e",
      "939be75c721f4f96abd162e29c15b929",
      "72276912ae4e4b2ea8ed65d3f78df5c3",
      "13eba33570614f1bba7f43015e5ab51d",
      "9b56baafc8bc4be2b97635bdaf83434b",
      "d893b8f1356e4da896b3d0e70ccf3196",
      "bc685139fc724421b18026415aaa3d0c"
     ]
    },
    "outputId": "7333a056-e5d8-4c58-de2c-72f3bcba25f9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u26a0\ufe0f Model doesn't have LoRA adapters. Adding them now...\n",
      "\u2713 LoRA adapters added successfully\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/1847 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3288a8ea33c24051a44e2944000f0441"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Trainer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Check if model has LoRA adapters\n",
    "if 'model' not in globals():\n",
    "    raise ValueError(\"Model not loaded. Please run the model loading cell first.\")\n",
    "\n",
    "# Check if model has peft config (LoRA adapters)\n",
    "if not hasattr(model, 'peft_config'):\n",
    "    print(\"\u26a0\ufe0f Model doesn't have LoRA adapters. Adding them now...\")\n",
    "    from unsloth import FastLanguageModel\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=64,  # LoRA rank - higher = more capacity, more memory\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
    "        lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "        bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
    "        random_state=3407,\n",
    "        use_rslora=False,  # Rank stabilized LoRA\n",
    "        loftq_config=None, # LoftQ\n",
    "    )\n",
    "    print(\"\u2713 LoRA adapters added successfully\")\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\u2713 Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vBGnx7DXuN9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "608c70e3-cba1-4a10-b9c6-b2673f7fdd7b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model prediction:\n",
      "<|user|> Predict the next number combination based on: 12, 34, 56, 78, 90<|end|><|assistant|>,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,',name_id_1',name_id_2',name_id_3',name_id_4',name_id_5',name_id_6',name_id_7',name_id_8',name_id_9',name_id_10',name_id_11,name_id_12',name_id_13\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model for number combination prediction\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Test prompt for number prediction\n",
    "# Adjust this based on your CSV data format\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Predict the next number combination based on: 12, 34, 56, 78, 90\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.3,  # Lower temperature for more deterministic predictions\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(\"Model prediction:\")\n",
    "print(response)\n",
    "\n",
    "# Extract just the prediction part\n",
    "if \"### Output:\" in response:\n",
    "    prediction = response.split(\"### Output:\")[-1].split(\"<|endoftext|>\")[0].strip()\n",
    "    print(f\"\\nExtracted prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twEmkIrLZLtD"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSmERd43la0l"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
    "if gguf_files:\n",
    "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
    "    print(f\"Downloading: {gguf_file}\")\n",
    "    files.download(gguf_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}