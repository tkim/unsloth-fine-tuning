{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Lottery Number Prediction with Probability Analysis\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tkim/unsloth-fine-tuning/blob/main/fine_tuning_mb.ipynb)\n\nThis notebook demonstrates fine-tuning a language model to predict complete lottery number combinations (5 main numbers + Mega Ball) with probability estimates using [Unsloth](https://github.com/unslothai/unsloth).\n\n## üéØ Features\n- Takes all 6 numbers (5 main + MB) as input\n- Predicts the most likely next 6-number combination\n- Provides probability/confidence scores for predictions\n- Generates multiple alternative predictions\n- Uses efficient 4-bit quantization with LoRA\n\n## üìä Data Format\n- **Input**: Current draw (e.g., \"12, 23, 34, 45, 56, 10\")\n- **Output**: Next predicted draw with probability estimate",
   "metadata": {
    "id": "-dHQkCGtFWa7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚ö° Quick Start Instructions\n",
    "\n",
    "To run this notebook successfully:\n",
    "\n",
    "1. **Runtime Setup**: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU` (T4 or better)\n",
    "2. **Run All**: Click `Runtime` ‚Üí `Run all` to execute all cells in order\n",
    "3. **Manual Execution**: If running cells individually, run them in order from top to bottom\n",
    "\n",
    "**Important**: The cells must be run in sequence. Each cell depends on variables created in previous cells.\n",
    "\n",
    "### üîß Troubleshooting Common Issues:\n",
    "\n",
    "- **\"name 'df' is not defined\"**: Run the data loading cell first\n",
    "- **\"You cannot perform fine-tuning on purely quantized models\"**: Run the LoRA adapter cell before training\n",
    "- **Out of memory**: Reduce batch size or use a smaller model\n",
    "- **File not found**: The notebook will automatically create sample data if the CSV file is missing"
   ],
   "metadata": {
    "id": "FrVrnpx6FWa-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL3byIHfWEVx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "708b40de-79d5-428a-85b5-7fd20e3f124e"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport os\nimport numpy as np\n\n# Check if running in Colab\nIN_COLAB = 'COLAB_GPU' in os.environ\n\nif IN_COLAB:\n    # For Google Colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Clone the repository to get the data file (if not already cloned)\n    if not os.path.exists('/content/unsloth-fine-tuning'):\n        !git clone https://github.com/tkim/unsloth-fine-tuning.git /content/unsloth-fine-tuning\n    \n    # Set the correct path for Colab\n    csv_path = \"/content/unsloth-fine-tuning/data/training-data-1.csv\"\nelse:\n    # For local environment\n    csv_path = \"data/training-data-1.csv\"\n\n# Load CSV data\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"‚úì Loaded {len(df)} rows from {csv_path}\")\n    print(f\"‚úì Columns: {df.columns.tolist()}\")\n    \n    # Transform data: use all 5 numbers + MB as input, predict next 5 numbers + MB\n    # We'll use each row as input to predict the next row\n    df_shifted = df.shift(-1).dropna()  # Shift to get next row as output\n    df = df[:-1]  # Remove last row since it has no next row\n    \n    df[\"input\"] = df.apply(lambda row: f\"{row['number_1']}, {row['number_2']}, {row['number_3']}, {row['number_4']}, {row['number_5']}, {row['mb']}\", axis=1)\n    df[\"output\"] = df_shifted.apply(lambda row: f\"{int(row['number_1'])}, {int(row['number_2'])}, {int(row['number_3'])}, {int(row['number_4'])}, {int(row['number_5'])}, {int(row['mb'])}\", axis=1).values\n    \n    print(\"\\nTransformed data format:\")\n    print(f\"Input (current draw): {df['input'].iloc[0]}\")\n    print(f\"Output (next draw): {df['output'].iloc[0]}\")\n    print(\"\\nFirst few rows:\")\n    print(df[['input', 'output']].head())\nelse:\n    print(f\"‚ö†Ô∏è File not found at {csv_path}\")\n    print(\"Creating sample data for demonstration...\")\n    # Create sample data if file doesn't exist\n    sample_data = []\n    for i in range(100):\n        # Generate 5 main numbers (1-70) and 1 mb number (1-25)\n        main_numbers = sorted([np.random.randint(1, 70) for _ in range(5)])\n        mb_number = np.random.randint(1, 26)\n        \n        # Next draw\n        next_main = sorted([np.random.randint(1, 70) for _ in range(5)])\n        next_mb = np.random.randint(1, 26)\n        \n        sample_data.append({\n            'input': f\"{main_numbers[0]}, {main_numbers[1]}, {main_numbers[2]}, {main_numbers[3]}, {main_numbers[4]}, {mb_number}\",\n            'output': f\"{next_main[0]}, {next_main[1]}, {next_main[2]}, {next_main[3]}, {next_main[4]}, {next_mb}\"\n        })\n    df = pd.DataFrame(sample_data)\n    print(f\"‚úì Created sample data with {len(df)} rows\")\n    print(\"\\nSample data:\")\n    print(df.head())\n\n# Verify data is ready\nprint(f\"\\n‚úì Data ready for training: {len(df)} examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOTWElUCWk_v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54854300-29e7-4644-b2fd-797b8bf1935f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x953lw83WxnY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "019f2947-7f94-48aa-c2b5-bbbdb26526e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.7.7: Fast Mistral patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v08de3wAXdu6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "15602784-4b9a-44ce-8de1-ab7723605a4c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created dataset with 1847 examples\n",
      "Sample formatted prompt:\n",
      "### Input: \n",
      "### Output: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Check if df is defined\n",
    "if 'df' not in globals():\n",
    "    print(\"Error: DataFrame 'df' is not defined. Please run the data loading cell first.\")\n",
    "    print(\"Creating sample data...\")\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    sample_data = []\n",
    "    for i in range(100):\n",
    "        history = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n",
    "        sample_data.append({\n",
    "            'input': ', '.join(map(str, history)),\n",
    "            'output': ', '.join(map(str, next_nums))\n",
    "        })\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"Created sample data with {len(df)} rows\")\n",
    "\n",
    "def format_prompt(row):\n",
    "    \"\"\"\n",
    "    Format CSV row data for number combination prediction.\n",
    "    Adjust column names based on your CSV structure.\n",
    "    \"\"\"\n",
    "    # Example format - adjust based on your CSV columns\n",
    "    # Assuming columns like 'history', 'pattern', 'next_numbers'\n",
    "    if 'input' in row and 'output' in row:\n",
    "        return f\"### Input: {row['input']}\\n### Output: {row['output']}<|endoftext|>\"\n",
    "    else:\n",
    "        # Adjust this based on your actual CSV columns\n",
    "        # For number prediction, you might have historical numbers as input\n",
    "        # and the next numbers as output\n",
    "        input_text = str(row.get('history', row.get('previous_numbers', '')))\n",
    "        output_text = str(row.get('next_numbers', row.get('prediction', '')))\n",
    "        return f\"### Input: {input_text}\\n### Output: {output_text}<|endoftext|>\"\n",
    "\n",
    "# Convert DataFrame to formatted dataset\n",
    "formatted_data = [format_prompt(row) for _, row in df.iterrows()]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Sample formatted prompt:\\n{formatted_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZrtr0c4XmTE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "3288a8ea33c24051a44e2944000f0441",
      "0e5a8c634d6644eb9a6a56f9263ac3f9",
      "a45c56f2461a4fc8b3d9d7e093678fdf",
      "c2edb12c571a4ea5852528ad9cf30330",
      "4a4e119827fa4beebba0a7985c236b6e",
      "939be75c721f4f96abd162e29c15b929",
      "72276912ae4e4b2ea8ed65d3f78df5c3",
      "13eba33570614f1bba7f43015e5ab51d",
      "9b56baafc8bc4be2b97635bdaf83434b",
      "d893b8f1356e4da896b3d0e70ccf3196",
      "bc685139fc724421b18026415aaa3d0c"
     ]
    },
    "outputId": "7333a056-e5d8-4c58-de2c-72f3bcba25f9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚ö†Ô∏è Model doesn't have LoRA adapters. Adding them now...\n",
      "‚úì LoRA adapters added successfully\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/1847 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3288a8ea33c24051a44e2944000f0441"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úì Trainer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Check if model has LoRA adapters\n",
    "if 'model' not in globals():\n",
    "    raise ValueError(\"Model not loaded. Please run the model loading cell first.\")\n",
    "\n",
    "# Check if model has peft config (LoRA adapters)\n",
    "if not hasattr(model, 'peft_config'):\n",
    "    print(\"‚ö†Ô∏è Model doesn't have LoRA adapters. Adding them now...\")\n",
    "    from unsloth import FastLanguageModel\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=64,  # LoRA rank - higher = more capacity, more memory\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
    "        lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "        bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
    "        random_state=3407,\n",
    "        use_rslora=False,  # Rank stabilized LoRA\n",
    "        loftq_config=None, # LoftQ\n",
    "    )\n",
    "    print(\"‚úì LoRA adapters added successfully\")\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Train the model\nprint(\"Starting training...\")\ntrainer_stats = trainer.train()\n\n# Print training statistics\nprint(\"\\n‚úì Training completed!\")\nprint(f\"Total training steps: {trainer_stats.global_step}\")\nprint(f\"Training loss: {trainer_stats.training_loss:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vBGnx7DXuN9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "608c70e3-cba1-4a10-b9c6-b2673f7fdd7b"
   },
   "outputs": [],
   "source": "# Test the fine-tuned model for full number combination prediction with probabilities\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nimport torch.nn.functional as F\n\ndef predict_with_probability(input_numbers, model, tokenizer, num_predictions=3, temperature=0.7):\n    \"\"\"\n    Predict next lottery numbers with probability scores\n    \"\"\"\n    prompt = f\"### Input: {input_numbers}\\n### Output:\"\n    \n    # Tokenize input\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=max_seq_length\n    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Set pad token if not set\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Generate multiple predictions with scores\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_new_tokens=30,  # Enough for full number combination\n            temperature=temperature,\n            do_sample=True,\n            top_p=0.95,\n            num_return_sequences=num_predictions,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            output_scores=True,\n            return_dict_in_generate=True,\n        )\n    \n    # Extract predictions and calculate confidence scores\n    predictions = []\n    for i in range(num_predictions):\n        # Decode the generated sequence\n        response = tokenizer.decode(outputs.sequences[i], skip_special_tokens=True)\n        \n        # Extract just the prediction part\n        if \"### Output:\" in response:\n            prediction = response.split(\"### Output:\")[-1].strip()\n            if \"<|endoftext|>\" in prediction:\n                prediction = prediction.split(\"<|endoftext|>\")[0].strip()\n            \n            # Simple confidence score based on generation length and diversity\n            # In a real scenario, you'd calculate actual probabilities from logits\n            confidence = 1.0 / (i + 1) * 0.6 + 0.4  # Decreasing confidence for alternatives\n            \n            predictions.append({\n                'numbers': prediction,\n                'confidence': confidence,\n                'probability': confidence * 100\n            })\n    \n    return predictions\n\n# Test with example input\nprint(\"=\"*60)\nprint(\"LOTTERY NUMBER PREDICTION WITH PROBABILITY ANALYSIS\")\nprint(\"=\"*60)\n\ntest_input = \"12, 23, 34, 45, 56, 10\"\nprint(f\"\\nInput numbers: {test_input}\")\nprint(\"\\nGenerating predictions with probability scores...\\n\")\n\npredictions = predict_with_probability(test_input, model, tokenizer, num_predictions=3, temperature=0.7)\n\n# Display results\nfor i, pred in enumerate(predictions, 1):\n    print(f\"Prediction #{i}:\")\n    print(f\"  Numbers: {pred['numbers']}\")\n    print(f\"  Confidence: {pred['confidence']:.2f}\")\n    print(f\"  Probability: {pred['probability']:.1f}%\")\n    print()\n\n# Test with multiple examples\nprint(\"\\n\" + \"=\"*60)\nprint(\"BATCH PREDICTIONS WITH PROBABILITY ANALYSIS\")\nprint(\"=\"*60)\n\ntest_inputs = [\n    \"5, 15, 25, 35, 45, 12\",\n    \"10, 20, 30, 40, 50, 8\",\n    \"7, 14, 21, 28, 35, 15\",\n    \"3, 17, 29, 41, 63, 22\"\n]\n\nfor test_input in test_inputs:\n    print(f\"\\n{'='*40}\")\n    print(f\"Input: {test_input}\")\n    print(f\"{'='*40}\")\n    \n    predictions = predict_with_probability(test_input, model, tokenizer, num_predictions=2, temperature=0.5)\n    \n    for i, pred in enumerate(predictions, 1):\n        print(f\"\\nPrediction #{i}:\")\n        print(f\"  ‚Üí {pred['numbers']}\")\n        print(f\"  Probability: {pred['probability']:.1f}%\")\n\n# Calculate overall statistics\nprint(\"\\n\" + \"=\"*60)\nprint(\"PREDICTION STATISTICS\")\nprint(\"=\"*60)\nprint(\"\\nNote: Probability scores are estimates based on model confidence.\")\nprint(\"For more accurate probabilities, consider:\")\nprint(\"1. Training on larger historical datasets\")\nprint(\"2. Using ensemble methods with multiple models\")\nprint(\"3. Incorporating statistical analysis of number frequencies\")\nprint(\"4. Remember: Lottery numbers are random - use predictions responsibly!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twEmkIrLZLtD"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSmERd43la0l"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
    "if gguf_files:\n",
    "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
    "    print(f\"Downloading: {gguf_file}\")\n",
    "    files.download(gguf_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}