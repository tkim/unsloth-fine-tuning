{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "import pandas as pd\nimport os\n\n# Check if running in Colab\nIN_COLAB = 'COLAB_GPU' in os.environ\n\nif IN_COLAB:\n    # For Google Colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Clone the repository to get the data file\n    !git clone https://github.com/tkim/unsloth-fine-tuning.git /content/unsloth-fine-tuning\n    \n    # Set the correct path for Colab\n    csv_path = \"/content/unsloth-fine-tuning/data/training-data-1.csv\"\nelse:\n    # For local environment\n    csv_path = \"data/training-data-1.csv\"\n\n# Check if file exists\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"Loaded {len(df)} rows from {csv_path}\")\n    print(f\"Columns: {df.columns.tolist()}\")\n    print(\"\\nFirst few rows:\")\n    print(df.head())\nelse:\n    print(f\"File not found at {csv_path}\")\n    print(\"Creating sample data...\")\n    # Create sample data if file doesn't exist\n    import numpy as np\n    sample_data = []\n    for i in range(100):\n        history = [np.random.randint(1, 100) for _ in range(5)]\n        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n        sample_data.append({\n            'input': ', '.join(map(str, history)),\n            'output': ', '.join(map(str, next_nums))\n        })\n    df = pd.DataFrame(sample_data)\n    print(f\"Created sample data with {len(df)} rows\")\n    print(df.head())",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ‚ö° Quick Start Instructions\n\nTo run this notebook successfully:\n\n1. **Runtime Setup**: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU` (T4 or better)\n2. **Run All**: Click `Runtime` ‚Üí `Run all` to execute all cells in order\n3. **Manual Execution**: If running cells individually, run them in order from top to bottom\n\n**Important**: The cells must be run in sequence. Each cell depends on variables created in previous cells.\n\n### üîß Troubleshooting Common Issues:\n\n- **\"name 'df' is not defined\"**: Run the data loading cell first\n- **\"You cannot perform fine-tuning on purely quantized models\"**: Run the LoRA adapter cell before training\n- **Out of memory**: Reduce batch size or use a smaller model\n- **File not found**: The notebook will automatically create sample data if the CSV file is missing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL3byIHfWEVx"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport os\nimport numpy as np\n\n# Check if running in Colab\nIN_COLAB = 'COLAB_GPU' in os.environ\n\nif IN_COLAB:\n    # For Google Colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Clone the repository to get the data file (if not already cloned)\n    if not os.path.exists('/content/unsloth-fine-tuning'):\n        !git clone https://github.com/tkim/unsloth-fine-tuning.git /content/unsloth-fine-tuning\n    \n    # Set the correct path for Colab\n    csv_path = \"/content/unsloth-fine-tuning/data/training-data-1.csv\"\nelse:\n    # For local environment\n    csv_path = \"data/training-data-1.csv\"\n\n# Check if file exists\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"‚úì Loaded {len(df)} rows from {csv_path}\")\n    print(f\"‚úì Columns: {df.columns.tolist()}\")\n    print(\"\\nFirst few rows:\")\n    print(df.head())\nelse:\n    print(f\"‚ö†Ô∏è File not found at {csv_path}\")\n    print(\"Creating sample data for demonstration...\")\n    # Create sample data if file doesn't exist\n    sample_data = []\n    for i in range(100):\n        history = [np.random.randint(1, 100) for _ in range(5)]\n        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n        sample_data.append({\n            'input': ', '.join(map(str, history)),\n            'output': ', '.join(map(str, next_nums))\n        })\n    df = pd.DataFrame(sample_data)\n    print(f\"‚úì Created sample data with {len(df)} rows\")\n    print(\"\\nSample data:\")\n    print(df.head())\n\n# Verify data is ready\nprint(f\"\\n‚úì Data ready for training: {len(df)} examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOTWElUCWk_v"
   },
   "outputs": [],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x953lw83WxnY"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIdADxFWXToO"
   },
   "outputs": [],
   "source": "from datasets import Dataset\n\ndef format_prompt(row):\n    \"\"\"\n    Format CSV row data for number combination prediction.\n    Adjust column names based on your CSV structure.\n    \"\"\"\n    # Example format - adjust based on your CSV columns\n    # Assuming columns like 'history', 'pattern', 'next_numbers'\n    if 'input' in row and 'output' in row:\n        return f\"### Input: {row['input']}\\n### Output: {row['output']}<|endoftext|>\"\n    else:\n        # Adjust this based on your actual CSV columns\n        # For number prediction, you might have historical numbers as input\n        # and the next numbers as output\n        input_text = str(row.get('history', row.get('previous_numbers', '')))\n        output_text = str(row.get('next_numbers', row.get('prediction', '')))\n        return f\"### Input: {input_text}\\n### Output: {output_text}<|endoftext|>\"\n\n# Convert DataFrame to formatted dataset\nformatted_data = [format_prompt(row) for _, row in df.iterrows()]\ndataset = Dataset.from_dict({\"text\": formatted_data})\n\nprint(f\"Created dataset with {len(dataset)} examples\")\nprint(f\"Sample formatted prompt:\\n{formatted_data[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v08de3wAXdu6"
   },
   "outputs": [],
   "source": "from datasets import Dataset\n\n# Check if df is defined\nif 'df' not in globals():\n    print(\"Error: DataFrame 'df' is not defined. Please run the data loading cell first.\")\n    print(\"Creating sample data...\")\n    import pandas as pd\n    import numpy as np\n    sample_data = []\n    for i in range(100):\n        history = [np.random.randint(1, 100) for _ in range(5)]\n        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n        sample_data.append({\n            'input': ', '.join(map(str, history)),\n            'output': ', '.join(map(str, next_nums))\n        })\n    df = pd.DataFrame(sample_data)\n    print(f\"Created sample data with {len(df)} rows\")\n\ndef format_prompt(row):\n    \"\"\"\n    Format CSV row data for number combination prediction.\n    Adjust column names based on your CSV structure.\n    \"\"\"\n    # Example format - adjust based on your CSV columns\n    # Assuming columns like 'history', 'pattern', 'next_numbers'\n    if 'input' in row and 'output' in row:\n        return f\"### Input: {row['input']}\\n### Output: {row['output']}<|endoftext|>\"\n    else:\n        # Adjust this based on your actual CSV columns\n        # For number prediction, you might have historical numbers as input\n        # and the next numbers as output\n        input_text = str(row.get('history', row.get('previous_numbers', '')))\n        output_text = str(row.get('next_numbers', row.get('prediction', '')))\n        return f\"### Input: {input_text}\\n### Output: {output_text}<|endoftext|>\"\n\n# Convert DataFrame to formatted dataset\nformatted_data = [format_prompt(row) for _, row in df.iterrows()]\ndataset = Dataset.from_dict({\"text\": formatted_data})\n\nprint(f\"Created dataset with {len(dataset)} examples\")\nprint(f\"Sample formatted prompt:\\n{formatted_data[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lm8booC8XliQ"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZrtr0c4XmTE"
   },
   "outputs": [],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Check if model has LoRA adapters\nif 'model' not in globals():\n    raise ValueError(\"Model not loaded. Please run the model loading cell first.\")\n\n# Check if model has peft config (LoRA adapters)\nif not hasattr(model, 'peft_config'):\n    print(\"‚ö†Ô∏è Model doesn't have LoRA adapters. Adding them now...\")\n    from unsloth import FastLanguageModel\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=64,  # LoRA rank - higher = more capacity, more memory\n        target_modules=[\n            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n            \"gate_proj\", \"up_proj\", \"down_proj\",\n        ],\n        lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n        lora_dropout=0,  # Supports any, but = 0 is optimized\n        bias=\"none\",     # Supports any, but = \"none\" is optimized\n        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n        random_state=3407,\n        use_rslora=False,  # Rank stabilized LoRA\n        loftq_config=None, # LoftQ\n    )\n    print(\"‚úì LoRA adapters added successfully\")\n\n# Training arguments optimized for Unsloth\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,  # Effective batch size = 8\n        warmup_steps=10,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=25,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        save_strategy=\"epoch\",\n        save_total_limit=2,\n        dataloader_pin_memory=False,\n        report_to=\"none\", # Disable Weights & Biases logging\n    ),\n)\n\nprint(\"‚úì Trainer initialized successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vBGnx7DXuN9"
   },
   "outputs": [],
   "source": "# Test the fine-tuned model for number combination prediction\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# Test prompt for number prediction\n# Adjust this based on your CSV data format\nmessages = [\n    {\"role\": \"user\", \"content\": \"Predict the next number combination based on: 12, 34, 56, 78, 90\"},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Generate response\noutputs = model.generate(\n    input_ids=inputs,\n    max_new_tokens=128,\n    use_cache=True,\n    temperature=0.3,  # Lower temperature for more deterministic predictions\n    do_sample=True,\n    top_p=0.9,\n)\n\n# Decode and print\nresponse = tokenizer.batch_decode(outputs)[0]\nprint(\"Model prediction:\")\nprint(response)\n\n# Extract just the prediction part\nif \"### Output:\" in response:\n    prediction = response.split(\"### Output:\")[-1].split(\"<|endoftext|>\")[0].strip()\n    print(f\"\\nExtracted prediction: {prediction}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twEmkIrLZLtD"
   },
   "outputs": [],
   "source": "# Test the fine-tuned model for number combination prediction\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# Test with the same format used during training\ntest_input = \"12, 34, 56, 78, 90\"\nprompt = f\"### Input: {test_input}\\n### Output:\"\n\n# Tokenize without chat template (use the same format as training)\ninputs = tokenizer(\n    prompt,\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True,\n    max_length=max_seq_length\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Set pad token if not set\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Generate response\nprint(\"Generating prediction...\")\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=50,  # Reduced for number predictions\n    use_cache=True,\n    temperature=0.3,  # Lower temperature for more deterministic predictions\n    do_sample=True,\n    top_p=0.9,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\n# Decode and print\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"\\nFull response:\")\nprint(response)\n\n# Extract just the prediction part\nif \"### Output:\" in response:\n    prediction = response.split(\"### Output:\")[-1].strip()\n    # Remove any remaining prompt text\n    if \"<|endoftext|>\" in prediction:\n        prediction = prediction.split(\"<|endoftext|>\")[0].strip()\n    print(f\"\\nExtracted prediction: {prediction}\")\nelse:\n    print(\"\\nCould not extract prediction. The model may need more training.\")\n\n# Alternative: Try a few more examples\nprint(\"\\n\" + \"=\"*50)\nprint(\"Testing with more examples:\")\ntest_inputs = [\n    \"1, 2, 3, 4, 5\",\n    \"10, 20, 30, 40, 50\",\n    \"5, 10, 15, 20, 25\"\n]\n\nfor test_input in test_inputs:\n    prompt = f\"### Input: {test_input}\\n### Output:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    outputs = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_new_tokens=30,\n        temperature=0.3,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if \"### Output:\" in response:\n        prediction = response.split(\"### Output:\")[-1].split(\"<|endoftext|>\")[0].strip()\n        print(f\"\\nInput: {test_input}\")\n        print(f\"Prediction: {prediction}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### üìä Model Testing & Evaluation\n\n**Note**: The quality of predictions depends on:\n- Quality and size of training data\n- Number of training epochs (default: 3)\n- Whether the training data has actual patterns vs random numbers\n\nIf predictions are poor, consider:\n1. Training for more epochs (increase `num_train_epochs`)\n2. Using real data with actual patterns\n3. Adjusting the model parameters\n4. Using a larger model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSmERd43la0l"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
    "if gguf_files:\n",
    "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
    "    print(f\"Downloading: {gguf_file}\")\n",
    "    files.download(gguf_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}