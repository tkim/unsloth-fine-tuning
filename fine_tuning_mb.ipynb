{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "import pandas as pd\nimport os\n\n# Check if running in Colab\nIN_COLAB = 'COLAB_GPU' in os.environ\n\nif IN_COLAB:\n    # For Google Colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Clone the repository to get the data file\n    !git clone https://github.com/tkim/unsloth-fine-tuning.git /content/unsloth-fine-tuning\n    \n    # Set the correct path for Colab\n    csv_path = \"/content/unsloth-fine-tuning/data/training-data-1.csv\"\nelse:\n    # For local environment\n    csv_path = \"data/training-data-1.csv\"\n\n# Check if file exists\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"Loaded {len(df)} rows from {csv_path}\")\n    print(f\"Columns: {df.columns.tolist()}\")\n    print(\"\\nFirst few rows:\")\n    print(df.head())\nelse:\n    print(f\"File not found at {csv_path}\")\n    print(\"Creating sample data...\")\n    # Create sample data if file doesn't exist\n    import numpy as np\n    sample_data = []\n    for i in range(100):\n        history = [np.random.randint(1, 100) for _ in range(5)]\n        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n        sample_data.append({\n            'input': ', '.join(map(str, history)),\n            'output': ', '.join(map(str, next_nums))\n        })\n    df = pd.DataFrame(sample_data)\n    print(f\"Created sample data with {len(df)} rows\")\n    print(df.head())",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL3byIHfWEVx"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport os\nimport numpy as np\n\n# Check if running in Colab\nIN_COLAB = 'COLAB_GPU' in os.environ\n\nif IN_COLAB:\n    # For Google Colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Clone the repository to get the data file (if not already cloned)\n    if not os.path.exists('/content/unsloth-fine-tuning'):\n        !git clone https://github.com/tkim/unsloth-fine-tuning.git /content/unsloth-fine-tuning\n    \n    # Set the correct path for Colab\n    csv_path = \"/content/unsloth-fine-tuning/data/training-data-1.csv\"\nelse:\n    # For local environment\n    csv_path = \"data/training-data-1.csv\"\n\n# Check if file exists\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"✓ Loaded {len(df)} rows from {csv_path}\")\n    print(f\"✓ Columns: {df.columns.tolist()}\")\n    print(\"\\nFirst few rows:\")\n    print(df.head())\nelse:\n    print(f\"⚠️ File not found at {csv_path}\")\n    print(\"Creating sample data for demonstration...\")\n    # Create sample data if file doesn't exist\n    sample_data = []\n    for i in range(100):\n        history = [np.random.randint(1, 100) for _ in range(5)]\n        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n        sample_data.append({\n            'input': ', '.join(map(str, history)),\n            'output': ', '.join(map(str, next_nums))\n        })\n    df = pd.DataFrame(sample_data)\n    print(f\"✓ Created sample data with {len(df)} rows\")\n    print(\"\\nSample data:\")\n    print(df.head())\n\n# Verify data is ready\nprint(f\"\\n✓ Data ready for training: {len(df)} examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOTWElUCWk_v"
   },
   "outputs": [],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x953lw83WxnY"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIdADxFWXToO"
   },
   "outputs": [],
   "source": "from datasets import Dataset\n\ndef format_prompt(row):\n    \"\"\"\n    Format CSV row data for number combination prediction.\n    Adjust column names based on your CSV structure.\n    \"\"\"\n    # Example format - adjust based on your CSV columns\n    # Assuming columns like 'history', 'pattern', 'next_numbers'\n    if 'input' in row and 'output' in row:\n        return f\"### Input: {row['input']}\\n### Output: {row['output']}<|endoftext|>\"\n    else:\n        # Adjust this based on your actual CSV columns\n        # For number prediction, you might have historical numbers as input\n        # and the next numbers as output\n        input_text = str(row.get('history', row.get('previous_numbers', '')))\n        output_text = str(row.get('next_numbers', row.get('prediction', '')))\n        return f\"### Input: {input_text}\\n### Output: {output_text}<|endoftext|>\"\n\n# Convert DataFrame to formatted dataset\nformatted_data = [format_prompt(row) for _, row in df.iterrows()]\ndataset = Dataset.from_dict({\"text\": formatted_data})\n\nprint(f\"Created dataset with {len(dataset)} examples\")\nprint(f\"Sample formatted prompt:\\n{formatted_data[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v08de3wAXdu6"
   },
   "outputs": [],
   "source": "from datasets import Dataset\n\n# Check if df is defined\nif 'df' not in globals():\n    print(\"Error: DataFrame 'df' is not defined. Please run the data loading cell first.\")\n    print(\"Creating sample data...\")\n    import pandas as pd\n    import numpy as np\n    sample_data = []\n    for i in range(100):\n        history = [np.random.randint(1, 100) for _ in range(5)]\n        next_nums = [np.random.randint(1, 100) for _ in range(5)]\n        sample_data.append({\n            'input': ', '.join(map(str, history)),\n            'output': ', '.join(map(str, next_nums))\n        })\n    df = pd.DataFrame(sample_data)\n    print(f\"Created sample data with {len(df)} rows\")\n\ndef format_prompt(row):\n    \"\"\"\n    Format CSV row data for number combination prediction.\n    Adjust column names based on your CSV structure.\n    \"\"\"\n    # Example format - adjust based on your CSV columns\n    # Assuming columns like 'history', 'pattern', 'next_numbers'\n    if 'input' in row and 'output' in row:\n        return f\"### Input: {row['input']}\\n### Output: {row['output']}<|endoftext|>\"\n    else:\n        # Adjust this based on your actual CSV columns\n        # For number prediction, you might have historical numbers as input\n        # and the next numbers as output\n        input_text = str(row.get('history', row.get('previous_numbers', '')))\n        output_text = str(row.get('next_numbers', row.get('prediction', '')))\n        return f\"### Input: {input_text}\\n### Output: {output_text}<|endoftext|>\"\n\n# Convert DataFrame to formatted dataset\nformatted_data = [format_prompt(row) for _, row in df.iterrows()]\ndataset = Dataset.from_dict({\"text\": formatted_data})\n\nprint(f\"Created dataset with {len(dataset)} examples\")\nprint(f\"Sample formatted prompt:\\n{formatted_data[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lm8booC8XliQ"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\", # Disable Weights & Biases logging\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZrtr0c4XmTE"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vBGnx7DXuN9"
   },
   "outputs": [],
   "source": "# Test the fine-tuned model for number combination prediction\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# Test prompt for number prediction\n# Adjust this based on your CSV data format\nmessages = [\n    {\"role\": \"user\", \"content\": \"Predict the next number combination based on: 12, 34, 56, 78, 90\"},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Generate response\noutputs = model.generate(\n    input_ids=inputs,\n    max_new_tokens=128,\n    use_cache=True,\n    temperature=0.3,  # Lower temperature for more deterministic predictions\n    do_sample=True,\n    top_p=0.9,\n)\n\n# Decode and print\nresponse = tokenizer.batch_decode(outputs)[0]\nprint(\"Model prediction:\")\nprint(response)\n\n# Extract just the prediction part\nif \"### Output:\" in response:\n    prediction = response.split(\"### Output:\")[-1].split(\"<|endoftext|>\")[0].strip()\n    print(f\"\\nExtracted prediction: {prediction}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twEmkIrLZLtD"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSmERd43la0l"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
    "if gguf_files:\n",
    "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
    "    print(f\"Downloading: {gguf_file}\")\n",
    "    files.download(gguf_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}